{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86bfeb12-7459-421b-97bc-f8cb8c9e7724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_gbq as pd_gbq\n",
    "import gc\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "from matplotlib import pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "import glob\n",
    "import lightgbm as lgb\n",
    "from google.cloud import storage\n",
    "from pandas_profiling import ProfileReport\n",
    "#from pyspark.context import SparkContext\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.ml.regression import LinearRegression\n",
    "#from pyspark.sql.session import SparkSession\n",
    "\n",
    "%reload_ext google.cloud.bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64aac43-dd9f-4287-861d-967494ba0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage before optimization is: {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#Train data\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "query_string = \"\"\"\n",
    "SELECT * FROM `crack-petal-363512.Amex.Train`\n",
    "\"\"\"\n",
    "\n",
    "#select a.*,b.target from\n",
    "#(SELECT * FROM `amexcds4.amextaiwan.amextaiwantrain`) a\n",
    "#left join\n",
    "#(SELECT * FROM `amexcds4.amextaiwan.trainLabels`) b\n",
    "#on a.customer_ID\t = b.customer_ID;\n",
    "\n",
    "dataframe = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(\n",
    "        # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "        # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "        # API is used by default.\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36df037-fa19-45d4-aa86-d5345b67448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train label\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "query_string = \"\"\"\n",
    "SELECT * FROM `crack-petal-363512.Amex.Train_Label`\n",
    "\"\"\"\n",
    "\n",
    "dataframe_labels = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(\n",
    "        # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "        # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "        # API is used by default.\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a3edf4-bbc8-4fe4-bb77-66337a590349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization is: 8018.31 MB\n",
      "Memory usage after optimization is: 2125.91 MB\n",
      "Decreased by 73.5%\n"
     ]
    }
   ],
   "source": [
    "# Reduce memory\n",
    "df=reduce_mem_usage(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049e7ffa-1989-4e8d-86cd-928e3b740711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del dataframe\n",
    "del dataframe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ac8b70-f347-4d5e-ad5d-b5f3ca58c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove correlated columns > 87%\n",
    "df.drop(['D_62'],axis=1,inplace=True) #D_77\n",
    "\n",
    "df.drop(['D_103'],axis=1,inplace=True) #D_104\n",
    "df.drop(['D_107'],axis=1,inplace=True)\n",
    "\n",
    "df.drop(['D_139'],axis=1,inplace=True) #D_143\n",
    "df.drop(['D_141'],axis=1,inplace=True)\n",
    "\n",
    "df.drop(['B_1'],axis=1,inplace=True) #B_37\n",
    "df.drop(['B_11'],axis=1,inplace=True)\n",
    "\n",
    "df.drop(['B_7'],axis=1,inplace=True) #B_23\n",
    "\n",
    "df.drop(['D_118'],axis=1,inplace=True)#D_119\n",
    "df.drop(['D_115'],axis=1,inplace=True)\n",
    "\n",
    "df.drop(['D_74'],axis=1,inplace=True)#D_75\n",
    "df.drop(['D_58'],axis=1,inplace=True)\n",
    "\n",
    "df.drop(['B_2'],axis=1,inplace=True)#B_33\n",
    "df.drop(['B_18'],axis=1,inplace=True)\n",
    "\n",
    "df.drop(['B_14'],axis=1,inplace=True)#B_15\n",
    "\n",
    "df.drop(['B_16'],axis=1,inplace=True)#B_20\n",
    "\n",
    "df.drop(['D_132'],axis=1,inplace=True)#D_131\n",
    "df.drop(['D_79'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "df.drop(['D_55'],axis=1,inplace=True)#D_48\n",
    "df.drop(['D_61'],axis=1,inplace=True)\n",
    "\n",
    "cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "\n",
    "for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        df[cat_col] = encoder.fit_transform(df[cat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7536737a-5cc4-484c-a658-aba054d9087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def floorify_frac(x, T=1):\n",
    "    \"\"\"convert to int if float appears ordinal\"\"\"\n",
    "    xt = (np.floor(x*T+1e-6)).fillna(-1)\n",
    "    if np.max(xt)<=127:\n",
    "        return xt.astype(np.int8)\n",
    "    return xt.astype(np.int16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc18dc42-8b83-4e63-9ab9-30541dfbba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floorify B_4\n",
      "floorify D_49\n",
      "floorify D_51\n",
      "floorify R_3\n",
      "floorify R_2\n",
      "floorify D_59\n",
      "floorify D_65\n",
      "floorify B_20\n",
      "floorify B_22\n",
      "floorify D_70\n",
      "floorify D_72\n",
      "floorify S_15\n",
      "floorify D_75\n",
      "floorify D_78\n",
      "floorify D_80\n",
      "floorify R_13\n",
      "floorify D_84\n",
      "floorify R_16\n",
      "floorify R_17\n",
      "floorify R_18\n",
      "floorify D_89\n",
      "floorify D_91\n",
      "floorify D_111\n",
      "floorify D_122\n",
      "floorify D_124\n",
      "floorify B_8\n",
      "floorify S_6\n",
      "floorify R_4\n",
      "floorify R_10\n",
      "floorify D_81\n",
      "floorify R_11\n",
      "floorify R_8\n",
      "floorify D_83\n",
      "floorify S_18\n",
      "floorify D_86\n",
      "floorify R_19\n",
      "floorify B_32\n",
      "floorify S_20\n",
      "floorify R_20\n",
      "floorify R_21\n",
      "floorify B_33\n",
      "floorify R_22\n",
      "floorify R_23\n",
      "floorify D_92\n",
      "floorify D_93\n",
      "floorify D_94\n",
      "floorify R_24\n",
      "floorify R_25\n",
      "floorify D_96\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = {\n",
    " \"B_4\" : 78,\n",
    " \"D_49\" : 71,\n",
    " \"D_51\" : 3,\n",
    " \"R_3\" : 10,\n",
    " \"R_2\":1,\n",
    " \"D_59\" : 48,\n",
    " \"D_65\": 38,\n",
    " \"D_59\":48,\n",
    " \"B_20\" : 17,\n",
    " \"B_22\" : 2,\n",
    " \"D_70\" : 4,\n",
    " \"D_72\" : 3,\n",
    " \"S_15\" : 10,\n",
    " \"D_75\" : 15,\n",
    " \"D_78\": 2,\n",
    " \"D_80\":5,\n",
    " \"R_13\":31,\n",
    " \"D_84\" : 2,\n",
    " \"R_16\": 2,\n",
    " \"R_17\": 35,\n",
    " \"R_18\": 31,\n",
    " \"D_89\" : 9,\n",
    " \"D_91\" : 2,\n",
    " \"D_111\" : 2,\n",
    " \"D_122\": 7,\n",
    " \"D_124\": 22,\n",
    " \"B_8\": 1,\n",
    " \"S_6\" : 1,\n",
    " \"R_4\": 1,\n",
    " \"R_10\": 1,\n",
    " \"D_81\":1,\n",
    " \"R_11\": 2,\n",
    " \"R_8\":1,\n",
    " 'D_83':1,\n",
    " 'S_18':1,\n",
    " 'D_86':1,\n",
    " 'R_19':1,\n",
    " 'B_32':1,\n",
    " 'S_20':1,\n",
    " 'R_20':1,\n",
    " 'R_21':1,\n",
    " 'B_33':1,\n",
    " 'R_22':1,\n",
    " 'R_23':1,\n",
    " 'D_92':1,\n",
    " 'D_93':1,\n",
    " 'D_94':1,\n",
    " 'R_24':1,\n",
    " 'R_25':1,\n",
    " 'D_96':1\n",
    "    \n",
    "}\n",
    "\n",
    "for var, rate in sampling_rate.items():\n",
    "    print(\"floorify\", var)\n",
    "    df[var] = floorify_frac(df[var],rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cddc690d-416a-4554-bde7-fc51fb552446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for null values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Remove Null values columns greater than 80% and for rest of null fill na ith median\n",
    "print(\"Checking for null values\\n\")\n",
    "null_col=[]\n",
    "null_col_val=[]\n",
    "null_col_less_than_80=[]\n",
    "null_col_less_than_80_val=[]\n",
    "null_col_less_than_50=[]\n",
    "null_col_less_than_50_val=[]\n",
    "null_col_less_than_10=[]\n",
    "null_col_less_than_10_val=[]\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    tot_null = pd.isnull(df[col]).sum()\n",
    "    pct_null = (float(tot_null) / df.shape[0]) * 100\n",
    "    if pct_null >= 80:\n",
    "       null_col.append(col)\n",
    "       null_col_val.append(pct_null)\n",
    "    if pct_null < 80 and pct_null >=50:\n",
    "        null_col_less_than_80.append(col)\n",
    "        null_col_less_than_80_val.append(pct_null)\n",
    "    if pct_null < 50 and pct_null>10 :\n",
    "        null_col_less_than_50.append(col)\n",
    "        null_col_less_than_50_val.append(pct_null)    \n",
    "    if pct_null < 10 and pct_null>0 :\n",
    "        null_col_less_than_10.append(col)\n",
    "        null_col_less_than_10_val.append(pct_null)   \n",
    "\n",
    "\n",
    "for num_col in null_col_less_than_10:\n",
    "    df[num_col].fillna((df[num_col].median()), inplace=True)\n",
    "\n",
    "for num_col in null_col_less_than_50:\n",
    "    df[num_col].fillna((df[num_col].median()), inplace=True)\n",
    "\n",
    "for num_col in null_col_less_than_80:\n",
    "    df[num_col].fillna((df[num_col].median()), inplace=True)\n",
    "\n",
    "for num_col in null_col:\n",
    "    df.drop(num_col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79bdec4b-fead-4ed9-86ea-45888e3f351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change S_2 to date\n",
    "df['S_2']=pd.to_datetime(df['S_2'])\n",
    "df = df.sort_values(by=['customer_ID','S_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8e37f3a-7723-46bc-b175-6297f6aa851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "#scaler = StandardScaler()\n",
    "#df=scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408effc0-ef9b-4a8c-a34e-aa94d9a8aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labels\n",
    "df=df.merge(dataframe_labels, how='inner', on='customer_ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82bf43f9-5847-41af-8357-caff731e37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:02<00:00, 32.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# To save as parquet we need to convert float to 32\n",
    "num_cols = list(df.dtypes[(df.dtypes == 'float32') | (df.dtypes == 'float16')].index)\n",
    "for col in tqdm(num_cols):\n",
    "    df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3133673-9d12-4d86-a262-231945caf6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to parquet\n",
    "table = pa.Table.from_pandas(df)\n",
    "pqwriter = pq.ParquetWriter(\"train_bucketted.parquet\", table.schema) \n",
    "pqwriter.write_table(table)\n",
    "pqwriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8b931-e6f3-4391-a9c3-5f4a90b98692",
   "metadata": {},
   "source": [
    "### read parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5713b79-b9ae-4954-beec-45a3b611a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet(\"train_bucketted.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daed0fc1-89f2-4eab-842d-4a86153ac96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('target',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a694b19f-b694-4dce-9a22-6a36fea89545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "features = df.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "num_features = [col for col in features if col not in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eb2e66b-afd2-4a88-806b-538e022c5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer id numeric columns and do mean, std, min, max, last\n",
    "df_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "df_num_agg.columns = ['_'.join(x) for x in df_num_agg.columns]\n",
    "df_num_agg.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614073bc-b496-4612-b13d-504020d72686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems there will be some null due to std column as there 5120 rows where there is only one\n",
    "#customer id row\n",
    "df_num_agg.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a7b4a34-44b4-4b90-a578-98c5a91cb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer id categorical columns and do count last and nunique\n",
    "df_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "df_cat_agg.columns = ['_'.join(x) for x in df_cat_agg.columns]\n",
    "df_cat_agg.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4238a4-8269-4cd8-a7df-71974813c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 13.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# convery to float 32 numeric columns\n",
    "cols = list(df_num_agg.dtypes[df_num_agg.dtypes == 'float64'].index)\n",
    "for col in tqdm(cols):\n",
    "    df_num_agg[col] = df_num_agg[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beb4470a-778e-45f5-8da6-fe8df982b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 71.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to int32 categorical columns\n",
    "cols = list(df_cat_agg.dtypes[df_cat_agg.dtypes == 'int64'].index)  \n",
    "for col in tqdm(cols):\n",
    "    df_cat_agg[col] = df_cat_agg[col].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3e2f883-4dc5-4f12-a221-e27c60df5f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458913/458913 [08:20<00:00, 917.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# do difference of current row with the next row and pick the last row \n",
    "df1 = []\n",
    "customer_ids = []\n",
    "for customer_id, df2 in tqdm(df.groupby(['customer_ID'])):\n",
    "    diff_df1 = df2[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "    df1.append(diff_df1)\n",
    "    customer_ids.append(customer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1f941a-3168-4871-8679-ce85ba6ce58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add _diff to columns where difference was calculated in the earlier step\n",
    "df1=np.concatenate(df1, axis = 0)\n",
    "df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df2[num_features].columns])\n",
    "df1['customer_ID'] = customer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f7dfe5c-ce8e-45a1-b24a-245539ea2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems there will be some null due to std column as there 5120 rows where there is only one\n",
    "#customer id row\n",
    "df1.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b2e3eb-5ce6-4e76-8a96-38675beb0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all numeric, categorical and difference dataframes\n",
    "#df = df_num_agg.merge(df_cat_agg, how = 'inner', on = 'customer_ID').merge(df1, how = 'inner', on = 'customer_ID').merge(dataframe_labels, how = 'inner', on = 'customer_ID')\n",
    "df = df_num_agg.merge(df_cat_agg, how = 'inner', on = 'customer_ID').merge(df1, how = 'inner', on = 'customer_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c40024f-8077-4fb7-84f0-fadff93903a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round all numeroc columns\n",
    "num_cols = list(df.dtypes[(df.dtypes == 'float32') | (df.dtypes == 'float64')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "752dd0d8-b19a-4bf0-ad12-df495c4b6539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for col in num_cols:\n",
    "    df[col + '_round2'] = df[col].round(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38840255-5d83-4067-9c52-f74ff7f47d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in df.columns if 'last' in col]\n",
    "num_cols = [col[:-5] for col in num_cols if 'round' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5c694-6d44-427c-9e73-2ace31ccda5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Create last minus mean difference column for all columns\n",
    "for col in num_cols:\n",
    "        try:\n",
    "            df[f'{col}_last_mean_diff'] = df[f'{col}_last'] - df[f'{col}_mean']\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d3ea0-5c1c-4df9-a768-4c353dc4b9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up all dataframes\n",
    "del df_num_agg, df_cat_agg, df1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6dce33-c63c-4567-bebe-0653ce060e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(dataframe_labels, how = 'inner', on = 'customer_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420cfc9-869b-43c4-a269-a2afd28fce3c",
   "metadata": {},
   "source": [
    "## This is the place where we convert the cleaned up dataframe to CSV or Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1316e79-bbbc-4a56-a0be-7843168d516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to CSV\n",
    "#df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44205e-4109-4b7a-85c6-2a972ed29f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1494/1494 [00:01<00:00, 1193.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to Parquet\n",
    "num_cols = list(df.dtypes[(df.dtypes == 'float32') | (df.dtypes == 'float16')].index)\n",
    "for col in tqdm(num_cols):\n",
    "    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "table = pa.Table.from_pandas(df)\n",
    "pqwriter = pq.ParquetWriter(\"train_bucketted_grouped.parquet\", table.schema) \n",
    "pqwriter.write_table(table)\n",
    "pqwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a10b4-0652-4409-ba71-a349ec1c4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
